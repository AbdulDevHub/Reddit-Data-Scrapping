{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59963d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install praw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4723eb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "# Initialize PRAW with your Reddit API credentials\n",
    "reddit = praw.Reddit(client_id='',\n",
    "                     client_secret='',\n",
    "                     user_agent='')\n",
    "\n",
    "# Let's define a function to scrape the top posts in a subreddit\n",
    "\n",
    "def scrape_subreddit(subreddit_name, num_posts):\n",
    "    # Fetch the specified subreddit\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    \n",
    "    # Fetch the top 'num_posts' posts from the subreddit for the year\n",
    "    top_posts_year = subreddit.top(limit=num_posts, time_filter='year')\n",
    "    \n",
    "    # List to store scraped data\n",
    "    data = []\n",
    "    \n",
    "    # Iterate through the top posts and store their titles, scores, number of comments, and content\n",
    "    for post in top_posts_year:\n",
    "        # Check if the post title or content contains relevant keywords (e.g., 'housing') <= This is only for more general subreddits like r/Canada\n",
    "        if 'housing' in post.title.lower() or 'housing' in post.selftext.lower():\n",
    "            post_data = {\n",
    "                \"Title\": post.title,\n",
    "                \"Score\": post.score,\n",
    "                \"Content\": post.selftext if post.selftext else None,\n",
    "                \"Number of Comments\": post.num_comments,\n",
    "                #\"Author\": post.author.name if post.author else None,\n",
    "                \"Post URL\": post.url,\n",
    "                \"Full URL\": f\"https://www.reddit.com{post.permalink}\"\n",
    "            }\n",
    "            data.append(post_data)\n",
    "    \n",
    "    return data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    subreddit_name = \"Canada\"\n",
    "    num_posts = 1000\n",
    "    \n",
    "    print(f\"\\nScraping top {num_posts} posts from r/{subreddit_name}...\\n\")\n",
    "    scraped_data = scrape_subreddit(subreddit_name, num_posts)\n",
    "    \n",
    "    # Create DataFrame from scraped data\n",
    "    df = pd.DataFrame(scraped_data)\n",
    "    \n",
    "    # Print the first 10 rows of the DataFrame\n",
    "    print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07faf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee7ddec",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0227d795",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Define stopwords and stemmer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "\n",
    "# Define a function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):\n",
    "        # Tokenize the text\n",
    "        tokens = word_tokenize(text)\n",
    "        # Convert tokens to lowercase and remove non-alphanumeric characters\n",
    "        tokens = [token.lower() for token in tokens if token.isalnum()]\n",
    "        # Remove stopwords\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "        # Stem the tokens\n",
    "        stemmed_tokens = [ps.stem(word) for word in tokens]\n",
    "        return stemmed_tokens\n",
    "    else:\n",
    "        return []  # Return an empty list for NaN values\n",
    "\n",
    "# Define the number of topics\n",
    "num_topics = 10\n",
    "\n",
    "# Define the number of top words to display for each topic\n",
    "n_top_words = 7\n",
    "\n",
    "# Apply preprocessing to the 'Content' column\n",
    "df['preprocessed_text'] = df['Content'].apply(preprocess_text)\n",
    "\n",
    "# Create a dictionary from the preprocessed text\n",
    "dictionary = corpora.Dictionary(df['preprocessed_text'])\n",
    "\n",
    "# Create a bag of words representation of the data (document-term matrix)\n",
    "corpus = [dictionary.doc2bow(tokens) for tokens in df['preprocessed_text']]\n",
    "\n",
    "# Run the LDA algorithm\n",
    "lda_model = LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=10)\n",
    "\n",
    "# Create a PDF to save the plot\n",
    "with PdfPages('topics_and_top_words.pdf') as pdf:\n",
    "    # Create subplots for each topic\n",
    "    plt.figure(figsize=(10, 20))\n",
    "    for topic_idx, topic in enumerate(lda_model.show_topics(num_topics=num_topics, num_words=n_top_words, formatted=False)):\n",
    "        # Extract the top words and their probabilities\n",
    "        top_words = [word for word, _ in topic[1]]\n",
    "        top_word_probs = [prob for _, prob in topic[1]]\n",
    "\n",
    "        # Plot the top words for the current topic\n",
    "        plt.subplot(5, 2, topic_idx + 1)  # Assuming 10 topics\n",
    "        plt.barh(top_words, top_word_probs, color='skyblue')\n",
    "        plt.gca().invert_yaxis()  # Invert y-axis for better readability\n",
    "        plt.title(f'Topic {topic_idx + 1}', fontsize=12)\n",
    "\n",
    "    # Add space between subplots\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the plot to the PDF\n",
    "    pdf.savefig()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297cee32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "# Define the number of top words to display for each topic\n",
    "n_top_words = 7\n",
    "\n",
    "# Get the average topic proportions\n",
    "average_topic_proportions = [sum(prob for _, prob in topic) / n_top_words for _, topic in lda_model.show_topics(num_topics=num_topics, num_words=n_top_words, formatted=False)]\n",
    "topic_labels = [f'Topic {i+1}' for i in range(len(average_topic_proportions))]\n",
    "\n",
    "# Create a PDF to save the plot\n",
    "with PdfPages('average_topic_proportions.pdf') as pdf:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(topic_labels, average_topic_proportions, color='skyblue')\n",
    "    plt.title('Average Topic Proportions', fontsize=16)\n",
    "    plt.xlabel('Topics', fontsize=14)\n",
    "    plt.ylabel('Average Proportion', fontsize=14)\n",
    "    plt.xticks(rotation=45, ha='right', fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    pdf.savefig()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6271f062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the topic number you want to search for\n",
    "topic_number = 3\n",
    "\n",
    "# Find documents associated with the specified topic\n",
    "topic_documents = []\n",
    "\n",
    "# Iterate through each document\n",
    "for i, doc in enumerate(corpus):\n",
    "    # Get the topic distribution for the current document\n",
    "    topics_distribution = lda_model.get_document_topics(doc)\n",
    "\n",
    "    # Check if the specified topic is one of the topics in the document\n",
    "    for topic, score in topics_distribution:\n",
    "        if topic == topic_number:\n",
    "            # Append the document index and topic score to the list\n",
    "            topic_documents.append((i, score))\n",
    "\n",
    "# Sort the documents by their topic score\n",
    "topic_documents.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the top 2 documents associated with the specified topic\n",
    "top_documents = 2  # Number of top documents to print\n",
    "for i, score in topic_documents[:top_documents]:\n",
    "    print(f\"Document {i + 1} (Score: {score}): {df['Full URL'][i]} \\n\") # <= Changed from content to URL for r/Canada since they posted images/links/videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6c219b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "\n",
    "# Prepare the visualization data\n",
    "vis_data = gensimvis.prepare(lda_model, corpus, dictionary)\n",
    "\n",
    "# Display the intertopic distance map\n",
    "pyLDAvis.display(vis_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
